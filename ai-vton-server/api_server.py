"""
IDM-VTON FastAPI Server (Production)
ê¸°ì¡´ IDM-VTON ëª¨ë¸ì„ FastAPIë¡œ ê°ì‹¸ì„œ ì œê³µ
ì„¤ì¹˜ ìœ„ì¹˜: ~/app/virtual-try/IDM-VTON/api_server.py

ìºì‹± ì „ëµ:
- ë©”ëª¨ë¦¬ ìºì‹œ ì—†ìŒ
- S3ì— ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ (UUID ê¸°ë°˜)
- NestJSê°€ ìºì‹œ ê´€ë¦¬ ì±…ì„
"""

import sys
import os

# Windowsì—ì„œ UTF-8 ì¶œë ¥ ì§€ì›
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# IDM-VTON gradio_demo ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
IDMVTON_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(IDMVTON_ROOT, "gradio_demo"))

# ============================================================================
# IDM-VTON ëª¨ë¸ ì´ˆê¸°í™”
# ============================================================================
print("=" * 80)
print("ğŸš€ Initializing IDM-VTON models...")
print("=" * 80)

with open("gradio_demo/app.py", "r") as f:
    app_code = f.read()

# ëª¨ë¸ ë¡œë”© ì½”ë“œ ì¶”ì¶œ ë° ì‹¤í–‰
init_code = app_code.split("def start_tryon")[0].split("garm_list = os.listdir")[0]
exec(init_code, globals())

# â­ CRITICAL: Deviceë¥¼ CUDAë¡œ ê°•ì œ ì„¤ì •
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"=" * 80)
print(f"ğŸ¯ Device explicitly set to: {device}")
print(f"=" * 80)

import numpy as np
import time
import io
import base64
from PIL import Image
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging
import boto3
from botocore.exceptions import ClientError
import asyncio

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… .env file loaded successfully")
except ImportError:
    print("âš ï¸  python-dotenv not installed, using system environment variables")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# S3 ì„¤ì • í™•ì¸
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-2")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
S3_BUCKET = os.getenv("AWS_S3_BUCKET", "your-bucket-name")

print("=" * 80)
print("ğŸ”‘ S3 Configuration Check:")
print(f"   Region: {AWS_REGION}")
print(f"   Bucket: {S3_BUCKET}")
print(f"   Access Key: {'âœ… Set' if AWS_ACCESS_KEY_ID else 'âŒ Missing'}")
print(f"   Secret Key: {'âœ… Set' if AWS_SECRET_ACCESS_KEY else 'âŒ Missing'}")
print("=" * 80)

# S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
s3_client = boto3.client(
    "s3",
    region_name=AWS_REGION,
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="IDM-VTON API Server", version="2.0.0")

# GPU ë™ì‹œì„± ì œì–´ (í•œ ë²ˆì— í•˜ë‚˜ì˜ diffusionë§Œ ì‹¤í–‰)
gpu_lock = asyncio.Lock()
request_queue_size = 0

# GPU ìµœì í™” í”Œë˜ê·¸
GPU_OPTIMIZATIONS_ENABLED = False

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# Request/Response Models
# ============================================================================


class HumanPreprocessRequest(BaseModel):
    user_id: str  # UUID
    image_base64: str


class VtonGenerateRequestV2(BaseModel):
    """FastAPIê°€ S3ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ìµœì í™” ë²„ì „)"""

    user_id: str  # UUID
    clothing_id: str  # UUID
    denoise_steps: int = 15
    seed: int = 42


class VtonBatchGenerateRequest(BaseModel):
    """ë°°ì¹˜ ì²˜ë¦¬ìš© - ì—¬ëŸ¬ ì˜·ì„ ë™ì‹œì— ì…ì–´ë³´ê¸°"""

    user_id: str
    clothing_ids: list[str]  # ì—¬ëŸ¬ ì˜· ID
    denoise_steps: int = 15
    seed: int = 42


class VtonBatchGenerateResponse(BaseModel):
    results: list[dict]  # [{clothing_id, result_image_base64, processing_time}, ...]
    total_processing_time: float


class HumanPreprocessResponse(BaseModel):
    user_id: str
    processing_time: float
    message: str
    human_img: str  # base64
    mask: str  # base64
    mask_gray: str  # base64
    pose_img_tensor: str  # base64 (pickled tensor)


class GarmentPreprocessRequest(BaseModel):
    user_id: str  # UUID
    clothing_id: str  # UUID
    image_base64: str


class GarmentPreprocessResponse(BaseModel):
    user_id: str
    clothing_id: str
    processing_time: float
    message: str
    garm_img: str  # base64
    garm_tensor: str  # base64 (pickled tensor)


class TextPreprocessRequest(BaseModel):
    user_id: str  # UUID
    clothing_id: str  # UUID
    garment_description: str


class TextPreprocessResponse(BaseModel):
    user_id: str
    clothing_id: str
    processing_time: float
    message: str
    prompt_embeds: str  # base64 (pickled tensor)
    negative_prompt_embeds: str  # base64 (pickled tensor)
    pooled_prompt_embeds: str  # base64 (pickled tensor)
    negative_pooled_prompt_embeds: str  # base64 (pickled tensor)
    prompt_embeds_c: str  # base64 (pickled tensor)


class VtonGenerateRequest(BaseModel):
    user_id: str  # UUID
    clothing_id: str  # UUID
    garment_description: str  # ìºì‹œëœ í…ìŠ¤íŠ¸ ì„ë² ë”© í‚¤
    denoise_steps: int = 15
    seed: int = 42
    # NestJSê°€ S3ì—ì„œ ë¡œë“œí•œ ìºì‹œ ë°ì´í„°
    human_img: str  # base64
    mask: str  # base64
    mask_gray: str  # base64
    pose_tensor: str  # base64 (pickled)
    garm_img: str  # base64
    garm_tensor: str  # base64 (pickled)
    prompt_embeds: str  # base64 (pickled)
    negative_prompt_embeds: str  # base64 (pickled)
    pooled_prompt_embeds: str  # base64 (pickled)
    negative_pooled_prompt_embeds: str  # base64 (pickled)
    prompt_embeds_c: str  # base64 (pickled)


class VtonGenerateResponse(BaseModel):
    result_image_base64: str
    processing_time: float


# ============================================================================
# Helper Functions
# ============================================================================


def base64_to_pil(base64_str: str) -> Image.Image:
    """Base64 â†’ PIL Image"""
    # data:image/png;base64, ì œê±°
    if "," in base64_str:
        base64_str = base64_str.split(",")[1]

    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image


def pil_to_base64(pil_img: Image.Image) -> str:
    """PIL Image â†’ Base64"""
    buffered = io.BytesIO()
    pil_img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def tensor_to_base64(tensor: torch.Tensor) -> str:
    """PyTorch Tensor â†’ Base64 (pickle ì§ë ¬í™”)"""
    import pickle

    buffer = io.BytesIO()
    pickle.dump(tensor.cpu(), buffer)
    buffer.seek(0)
    return base64.b64encode(buffer.read()).decode("utf-8")


def base64_to_tensor(base64_str: str, device_name: str = "cuda") -> torch.Tensor:
    """Base64 â†’ PyTorch Tensor (pickle ì—­ì§ë ¬í™”)"""
    import pickle

    buffer = io.BytesIO(base64.b64decode(base64_str))
    tensor = pickle.load(buffer)
    return tensor.to(device_name, torch.float16)


def download_from_s3(key: str) -> bytes:
    """S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = s3_client.get_object(Bucket=S3_BUCKET, Key=key)
        return response["Body"].read()
    except ClientError as e:
        logger.error(f"S3 download failed: {key} - {e}")
        raise HTTPException(status_code=404, detail=f"Cache not found in S3: {key}")


def download_s3_as_base64(key: str) -> str:
    """S3ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ Base64ë¡œ ë°˜í™˜"""
    data = download_from_s3(key)
    return base64.b64encode(data).decode("utf-8")


def download_s3_as_pil(key: str) -> Image.Image:
    """S3ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ PIL Imageë¡œ ë°˜í™˜"""
    data = download_from_s3(key)
    return Image.open(io.BytesIO(data))


def download_s3_as_tensor(key: str, device_name: str = "cuda") -> torch.Tensor:
    """S3ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ PyTorch Tensorë¡œ ë°˜í™˜ (pickle)"""
    import pickle

    data = download_from_s3(key)
    tensor = pickle.loads(data)
    return tensor.to(device_name, torch.float16)


# ============================================================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================


def preprocess_human_internal(human_img: Image.Image) -> dict:
    """
    ì‚¬ëŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: OpenPose + Parsing + DensePose

    Returns:
        {
            'human_img': base64,
            'mask': base64,
            'mask_gray': base64,
            'pose_img_tensor': base64 (pickled tensor)
        }
    """
    logger.info("â³ Preprocessing human image...")
    start = time.time()

    if isinstance(human_img, np.ndarray):
        human_img = Image.fromarray(human_img)

    # ì›ë³¸ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
    original_size = human_img.size
    target_width, target_height = 768, 1024

    # ë¹„ìœ¨ ê³„ì‚°
    aspect_ratio = original_size[0] / original_size[1]
    target_aspect = target_width / target_height

    if aspect_ratio > target_aspect:
        # ì´ë¯¸ì§€ê°€ ë” ë„“ìŒ -> í­ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        new_width = target_width
        new_height = int(target_width / aspect_ratio)
    else:
        # ì´ë¯¸ì§€ê°€ ë” ë†’ìŒ -> ë†’ì´ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        new_height = target_height
        new_width = int(target_height * aspect_ratio)

    # ë¦¬ì‚¬ì´ì¦ˆ í›„ ì¤‘ì•™ í¬ë¡­ ë˜ëŠ” íŒ¨ë”©
    human_img = human_img.convert("RGB").resize((new_width, new_height), Image.Resampling.LANCZOS)

    # 768x1024ë¡œ ì¤‘ì•™ í¬ë¡­ ë˜ëŠ” íŒ¨ë”©
    if new_width < target_width or new_height < target_height:
        # íŒ¨ë”© í•„ìš”
        padded_img = Image.new("RGB", (target_width, target_height), (255, 255, 255))
        paste_x = (target_width - new_width) // 2
        paste_y = (target_height - new_height) // 2
        padded_img.paste(human_img, (paste_x, paste_y))
        human_img = padded_img
    elif new_width > target_width or new_height > target_height:
        # í¬ë¡­ í•„ìš”
        left = (new_width - target_width) // 2
        top = (new_height - target_height) // 2
        human_img = human_img.crop((left, top, left + target_width, top + target_height))

    human_img_arg = _apply_exif_orientation(human_img.resize((384, 512)))
    human_img_arg = convert_PIL_to_numpy(human_img_arg, format="BGR")

    args = apply_net.create_argument_parser().parse_args(
        [
            "show",
            "./configs/densepose_rcnn_R_50_FPN_s1x.yaml",
            "./ckpt/densepose/model_final_162be9.pkl",
            "dp_segm",
            "-v",
            "--opts",
            "MODEL.DEVICE",
            "cuda",
        ]
    )

    # OpenPose
    keypoints = openpose_model(human_img.resize((384, 512)))

    # Parsing
    model_parse, _ = parsing_model(human_img.resize((384, 512)))
    mask, mask_gray = get_mask_location("hd", "upper_body", model_parse, keypoints)
    mask = mask.resize((768, 1024))

    # DensePose
    pose_img = args.func(args, human_img_arg)
    pose_img = pose_img[:, :, ::-1]
    pose_img = Image.fromarray(pose_img).resize((768, 1024))
    pose_img_tensor = tensor_transfrom(pose_img).unsqueeze(0).to(device, torch.float16)

    elapsed = time.time() - start
    logger.info(f"âœ… Human preprocessing completed in {elapsed:.2f}s")

    return {
        "human_img": pil_to_base64(human_img),
        "mask": pil_to_base64(mask),
        "mask_gray": pil_to_base64(mask_gray),
        "pose_img_tensor": tensor_to_base64(pose_img_tensor),
        "elapsed": elapsed,
    }


def preprocess_garment_internal(garm_img: Image.Image) -> dict:
    """
    ì˜· ì´ë¯¸ì§€ ì „ì²˜ë¦¬

    Returns:
        {
            'garm_img': base64,
            'garm_tensor': base64 (pickled tensor)
        }
    """
    logger.info("â³ Preprocessing garment image...")
    start = time.time()

    if isinstance(garm_img, np.ndarray):
        garm_img = Image.fromarray(garm_img)

    # ì›ë³¸ ë¹„ìœ¨ ìœ ì§€í•˜ë©° 768x1024ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    garm_img = garm_img.convert("RGB")
    original_size = garm_img.size
    target_width, target_height = 768, 1024

    # ë¹„ìœ¨ ê³„ì‚°
    aspect_ratio = original_size[0] / original_size[1]
    target_aspect = target_width / target_height

    if aspect_ratio > target_aspect:
        new_width = target_width
        new_height = int(target_width / aspect_ratio)
    else:
        new_height = target_height
        new_width = int(target_height * aspect_ratio)

    garm_img = garm_img.resize((new_width, new_height), Image.Resampling.LANCZOS)

    # ì¤‘ì•™ ì •ë ¬ë¡œ 768x1024 ë§ì¶”ê¸° (í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ íŒ¨ë”©)
    if new_width < target_width or new_height < target_height:
        padded_img = Image.new("RGB", (target_width, target_height), (255, 255, 255))
        paste_x = (target_width - new_width) // 2
        paste_y = (target_height - new_height) // 2
        padded_img.paste(garm_img, (paste_x, paste_y))
        garm_img = padded_img
    elif new_width > target_width or new_height > target_height:
        left = (new_width - target_width) // 2
        top = (new_height - target_height) // 2
        garm_img = garm_img.crop((left, top, left + target_width, top + target_height))

    garm_img_resized = garm_img.resize((384, 512))
    garm_tensor = (
        tensor_transfrom(garm_img_resized).unsqueeze(0).to(device, torch.float16)
    )

    elapsed = time.time() - start
    logger.info(f"âœ… Garment preprocessing completed in {elapsed:.2f}s")

    return {
        "garm_img": pil_to_base64(garm_img),
        "garm_tensor": tensor_to_base64(garm_tensor),
        "elapsed": elapsed,
    }


def preprocess_text_internal(garment_des: str) -> dict:
    """
    í…ìŠ¤íŠ¸ ì¸ì½”ë”©: CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©

    Returns:
        {
            'prompt_embeds': base64,
            'negative_prompt_embeds': base64,
            'pooled_prompt_embeds': base64,
            'negative_pooled_prompt_embeds': base64,
            'prompt_embeds_c': base64
        }
    """
    logger.info(f"â³ Encoding text: '{garment_des}'")
    start = time.time()

    # Gradio ìŠ¤íƒ€ì¼: NestJSì—ì„œ ì „ë‹¬ë°›ì€ ê°„ê²°í•œ ì„¤ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # ì˜ˆ: "Cardigan Gray Button" (ì´ë¯¸ NestJSì—ì„œ ê°„ê²°í•˜ê²Œ ì²˜ë¦¬ë¨)
    prompt = "model is wearing " + garment_des
    prompt_c = "a photo of " + garment_des
    negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

    # ğŸ“ ìƒì„±ë  í”„ë¡¬í”„íŠ¸ ì¶œë ¥
    print(f"ğŸ“ Text prompt: '{garment_des}'")

    with torch.no_grad():
        # text_encoderë§Œ float32ë¡œ ë³€í™˜ (test.pyì™€ ë™ì¼)
        original_dtype_1 = pipe.text_encoder.dtype
        original_dtype_2 = pipe.text_encoder_2.dtype
        pipe.text_encoder.to(torch.float32)
        pipe.text_encoder_2.to(torch.float32)

        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = pipe.encode_prompt(
            prompt,
            num_images_per_prompt=1,
            do_classifier_free_guidance=True,
            negative_prompt=negative_prompt,
        )

        prompt_embeds_c, _, _, _ = pipe.encode_prompt(
            prompt_c,
            num_images_per_prompt=1,
            do_classifier_free_guidance=False,
            negative_prompt=negative_prompt,
        )

        # ì›ë˜ dtypeìœ¼ë¡œ ë³µì› (test.pyì™€ ë™ì¼)
        pipe.text_encoder.to(original_dtype_1)
        pipe.text_encoder_2.to(original_dtype_2)

    elapsed = time.time() - start
    logger.info(f"âœ… Text encoding completed in {elapsed:.2f}s")

    return {
        "prompt_embeds": tensor_to_base64(prompt_embeds.to(device, torch.float16)),
        "negative_prompt_embeds": tensor_to_base64(
            negative_prompt_embeds.to(device, torch.float16)
        ),
        "pooled_prompt_embeds": tensor_to_base64(
            pooled_prompt_embeds.to(device, torch.float16)
        ),
        "negative_pooled_prompt_embeds": tensor_to_base64(
            negative_pooled_prompt_embeds.to(device, torch.float16)
        ),
        "prompt_embeds_c": tensor_to_base64(prompt_embeds_c.to(device, torch.float16)),
        "elapsed": elapsed,
    }


def generate_tryon_internal(
    human_img: Image.Image,
    mask: Image.Image,
    mask_gray: Image.Image,
    pose_img_tensor: torch.Tensor,
    garm_img: Image.Image,
    garm_tensor: torch.Tensor,
    prompt_embeds: torch.Tensor,
    negative_prompt_embeds: torch.Tensor,
    pooled_prompt_embeds: torch.Tensor,
    negative_pooled_prompt_embeds: torch.Tensor,
    prompt_embeds_c: torch.Tensor,
    denoise_steps: int,
    seed: int,
) -> tuple:
    """
    Diffusion ìƒì„± (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)

    Returns:
        (result_image: PIL.Image, elapsed: float)
    """
    logger.info("âš¡ Generating try-on with diffusion...")
    start = time.time()

    # Device ëª…ì‹œì  ì„¤ì • (CUDA ì‚¬ìš©)
    device_str = "cuda" if torch.cuda.is_available() else "cpu"

    # Gradio ìŠ¤íƒ€ì¼: autocast ì‚¬ìš©
    with torch.no_grad(), torch.cuda.amp.autocast():
        generator = torch.Generator(device=device_str).manual_seed(int(seed))

        images = pipe(
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            num_inference_steps=int(denoise_steps),
            generator=generator,
            strength=1.0,
            pose_img=pose_img_tensor,
            text_embeds_cloth=prompt_embeds_c,
            cloth=garm_tensor,
            mask_image=mask,
            image=human_img,
            height=1024,
            width=768,
            ip_adapter_image=garm_img,
            guidance_scale=2.0,
        )[0]

    elapsed = time.time() - start
    logger.info(f"âš¡ Diffusion completed in {elapsed:.2f}s")

    return images[0], elapsed


# ============================================================================
# API Endpoints
# ============================================================================


@app.get("/")
def root():
    return {
        "service": "IDM-VTON API Server",
        "version": "2.0.0",
        "status": "running",
        "port": 8001,
        "environment": "production",
        "models_loaded": True,
        "caching": "S3-based (NestJS managed)",
    }


@app.get("/health")
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {"status": "healthy", "models_loaded": True, "caching": "S3-based"}


@app.post("/vton/preprocess-human", response_model=HumanPreprocessResponse)
async def preprocess_human(request: HumanPreprocessRequest):
    """
    ì‚¬ëŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: OpenPose + Parsing + DensePose

    NestJSê°€ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ S3ì— ì €ì¥:
    - users/{user_id}/vton-cache/human_img.png
    - users/{user_id}/vton-cache/mask.png
    - users/{user_id}/vton-cache/mask_gray.png
    - users/{user_id}/vton-cache/pose_tensor.pkl
    """
    try:
        logger.info(f"[preprocess-human] user_id={request.user_id}")

        # Base64 â†’ PIL
        human_img = base64_to_pil(request.image_base64)

        # ì „ì²˜ë¦¬
        result = preprocess_human_internal(human_img)

        # NestJSê°€ S3ì— ì—…ë¡œë“œí•  ë°ì´í„° ë°˜í™˜
        return HumanPreprocessResponse(
            user_id=request.user_id,
            processing_time=result["elapsed"],
            message="Preprocessing completed",
            human_img=result["human_img"],
            mask=result["mask"],
            mask_gray=result["mask_gray"],
            pose_img_tensor=result["pose_img_tensor"],
        )
    except Exception as e:
        logger.error(f"[preprocess-human] Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/vton/preprocess-garment", response_model=GarmentPreprocessResponse)
async def preprocess_garment(request: GarmentPreprocessRequest):
    """
    ì˜· ì´ë¯¸ì§€ ì „ì²˜ë¦¬

    NestJSê°€ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ S3ì— ì €ì¥:
    - users/{user_id}/vton-cache/garments/{clothing_id}_img.png
    - users/{user_id}/vton-cache/garments/{clothing_id}_tensor.pkl
    """
    try:
        logger.info(
            f"[preprocess-garment] user_id={request.user_id}, clothing_id={request.clothing_id}"
        )

        # Base64 â†’ PIL
        garm_img = base64_to_pil(request.image_base64)

        # ì „ì²˜ë¦¬
        result = preprocess_garment_internal(garm_img)

        return GarmentPreprocessResponse(
            user_id=request.user_id,
            clothing_id=request.clothing_id,
            processing_time=result["elapsed"],
            message="Preprocessing completed",
            garm_img=result["garm_img"],
            garm_tensor=result["garm_tensor"],
        )
    except Exception as e:
        logger.error(f"[preprocess-garment] Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/vton/preprocess-text", response_model=TextPreprocessResponse)
async def preprocess_text(request: TextPreprocessRequest):
    """
    í…ìŠ¤íŠ¸ ì¸ì½”ë”©: CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©

    NestJSê°€ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ S3ì— ì €ì¥:
    - users/{user_id}/vton-cache/text/{clothing_id}_*.pkl
    """
    try:
        logger.info(
            f"[preprocess-text] user_id={request.user_id}, clothing_id={request.clothing_id}, text='{request.garment_description}'"
        )

        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        result = preprocess_text_internal(request.garment_description)

        return TextPreprocessResponse(
            user_id=request.user_id,
            clothing_id=request.clothing_id,
            processing_time=result["elapsed"],
            message="Text encoding completed",
            prompt_embeds=result["prompt_embeds"],
            negative_prompt_embeds=result["negative_prompt_embeds"],
            pooled_prompt_embeds=result["pooled_prompt_embeds"],
            negative_pooled_prompt_embeds=result["negative_pooled_prompt_embeds"],
            prompt_embeds_c=result["prompt_embeds_c"],
        )
    except Exception as e:
        logger.error(f"[preprocess-text] Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/vton/generate-tryon", response_model=VtonGenerateResponse)
async def generate_tryon(request: VtonGenerateRequestV2):
    """
    ìµœì í™” ë²„ì „: FastAPIê°€ S3ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ

    ì´ì :
    - NestJS â†’ FastAPI HTTP ì „ì†¡ ì œê±°
    - S3 ë‹¤ìš´ë¡œë“œ ë³‘ë ¬ ì²˜ë¦¬
    - ì˜ˆìƒ 2-3ì´ˆ ë‹¨ì¶•
    """
    global request_queue_size

    # í ì§„ì…
    request_queue_size += 1
    queue_position = request_queue_size
    logger.info(f"[generate-tryon-v2] Request queued (position: {queue_position})")

    try:
        # GPU Lock íšë“ (ëŒ€ê¸°)
        async with gpu_lock:
            logger.info(
                f"[generate-tryon-v2] Processing started - user_id={request.user_id}, clothing_id={request.clothing_id}"
            )
            start_time = time.time()

            # S3ì—ì„œ ìºì‹œ ë°ì´í„° ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
            logger.info("âš¡ Downloading cache from S3...")
            download_start = time.time()

            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor(max_workers=11) as executor:
                # S3 Key ìƒì„±
                user_id = request.user_id
                clothing_id = request.clothing_id

                futures = {
                    "human_img": executor.submit(
                        download_s3_as_pil, f"users/{user_id}/vton-cache/human_img.png"
                    ),
                    "mask": executor.submit(
                        download_s3_as_pil, f"users/{user_id}/vton-cache/mask.png"
                    ),
                    "mask_gray": executor.submit(
                        download_s3_as_pil, f"users/{user_id}/vton-cache/mask_gray.png"
                    ),
                    "pose_tensor": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/pose_tensor.pkl",
                        device,
                    ),
                    "garm_img": executor.submit(
                        download_s3_as_pil,
                        f"users/{user_id}/vton-cache/garments/{clothing_id}_img.png",
                    ),
                    "garm_tensor": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/garments/{clothing_id}_tensor.pkl",
                        device,
                    ),
                    "prompt_embeds": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/text/{clothing_id}_prompt_embeds.pkl",
                        device,
                    ),
                    "negative_prompt_embeds": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/text/{clothing_id}_negative_prompt_embeds.pkl",
                        device,
                    ),
                    "pooled_prompt_embeds": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/text/{clothing_id}_pooled_prompt_embeds.pkl",
                        device,
                    ),
                    "negative_pooled_prompt_embeds": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/text/{clothing_id}_negative_pooled_prompt_embeds.pkl",
                        device,
                    ),
                    "prompt_embeds_c": executor.submit(
                        download_s3_as_tensor,
                        f"users/{user_id}/vton-cache/text/{clothing_id}_prompt_embeds_c.pkl",
                        device,
                    ),
                }

                # ê²°ê³¼ ìˆ˜ì§‘
                cache_data = {key: future.result() for key, future in futures.items()}

            download_elapsed = time.time() - download_start
            logger.info(f"âœ… S3 download completed in {download_elapsed:.2f}s")

            # Diffusion ìƒì„±
            result_img, diffusion_elapsed = generate_tryon_internal(
                human_img=cache_data["human_img"],
                mask=cache_data["mask"],
                mask_gray=cache_data["mask_gray"],
                pose_img_tensor=cache_data["pose_tensor"],
                garm_img=cache_data["garm_img"],
                garm_tensor=cache_data["garm_tensor"],
                prompt_embeds=cache_data["prompt_embeds"],
                negative_prompt_embeds=cache_data["negative_prompt_embeds"],
                pooled_prompt_embeds=cache_data["pooled_prompt_embeds"],
                negative_pooled_prompt_embeds=cache_data["negative_pooled_prompt_embeds"],
                prompt_embeds_c=cache_data["prompt_embeds_c"],
                denoise_steps=request.denoise_steps,
                seed=request.seed,
            )

            total_elapsed = time.time() - start_time
            logger.info(
                f"ğŸ‰ Total: {total_elapsed:.2f}s (S3: {download_elapsed:.2f}s + Diffusion: {diffusion_elapsed:.2f}s)"
            )

            return VtonGenerateResponse(
                result_image_base64=pil_to_base64(result_img), processing_time=total_elapsed
            )

    except Exception as e:
        logger.error(f"[generate-tryon-v2] Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/vton/generate-batch", response_model=VtonBatchGenerateResponse)
async def generate_batch(request: VtonBatchGenerateRequest):
    """
    ë°°ì¹˜ ì²˜ë¦¬: í•œ ì‚¬ìš©ìê°€ ì—¬ëŸ¬ ì˜·ì„ ë™ì‹œì— ì…ì–´ë³´ê¸°

    GPU ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” í•œ ì—¬ëŸ¬ ì˜·ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
    """
    try:
        logger.info(
            f"[generate-batch] user_id={request.user_id}, {len(request.clothing_ids)} items"
        )
        start_time = time.time()

        results = []

        # ì‚¬ëŒ ìºì‹œëŠ” í•œ ë²ˆë§Œ ë¡œë“œ
        logger.info("Loading human cache...")
        user_id = request.user_id

        human_img = download_s3_as_pil(f"users/{user_id}/vton-cache/human_img.png")
        mask = download_s3_as_pil(f"users/{user_id}/vton-cache/mask.png")
        mask_gray = download_s3_as_pil(f"users/{user_id}/vton-cache/mask_gray.png")
        pose_tensor = download_s3_as_tensor(
            f"users/{user_id}/vton-cache/pose_tensor.pkl", device
        )

        # ê° ì˜·ì— ëŒ€í•´ ìˆœì°¨ ì²˜ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ìˆœì°¨)
        for clothing_id in request.clothing_ids:
            try:
                item_start = time.time()
                logger.info(f"Processing clothing_id={clothing_id}")

                # ì˜· ìºì‹œ ë¡œë“œ
                garm_img = download_s3_as_pil(
                    f"users/{user_id}/vton-cache/garments/{clothing_id}_img.png"
                )
                garm_tensor = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/garments/{clothing_id}_tensor.pkl",
                    device,
                )
                prompt_embeds = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/text/{clothing_id}_prompt_embeds.pkl",
                    device,
                )
                negative_prompt_embeds = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/text/{clothing_id}_negative_prompt_embeds.pkl",
                    device,
                )
                pooled_prompt_embeds = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/text/{clothing_id}_pooled_prompt_embeds.pkl",
                    device,
                )
                negative_pooled_prompt_embeds = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/text/{clothing_id}_negative_pooled_prompt_embeds.pkl",
                    device,
                )
                prompt_embeds_c = download_s3_as_tensor(
                    f"users/{user_id}/vton-cache/text/{clothing_id}_prompt_embeds_c.pkl",
                    device,
                )

                # Diffusion ìƒì„±
                result_img, _ = generate_tryon_internal(
                    human_img=human_img,
                    mask=mask,
                    mask_gray=mask_gray,
                    pose_img_tensor=pose_tensor,
                    garm_img=garm_img,
                    garm_tensor=garm_tensor,
                    prompt_embeds=prompt_embeds,
                    negative_prompt_embeds=negative_prompt_embeds,
                    pooled_prompt_embeds=pooled_prompt_embeds,
                    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
                    prompt_embeds_c=prompt_embeds_c,
                    denoise_steps=request.denoise_steps,
                    seed=request.seed,
                )

                item_elapsed = time.time() - item_start

                results.append(
                    {
                        "clothing_id": clothing_id,
                        "result_image_base64": pil_to_base64(result_img),
                        "processing_time": item_elapsed,
                        "success": True,
                    }
                )

                logger.info(
                    f"âœ… clothing_id={clothing_id} completed in {item_elapsed:.2f}s"
                )

            except Exception as item_error:
                logger.error(f"âŒ clothing_id={clothing_id} failed: {item_error}")
                results.append(
                    {
                        "clothing_id": clothing_id,
                        "result_image_base64": "",
                        "processing_time": 0,
                        "success": False,
                        "error": str(item_error),
                    }
                )

        total_elapsed = time.time() - start_time
        logger.info(
            f"ğŸ‰ Batch processing completed: {len(results)} items in {total_elapsed:.2f}s"
        )

        return VtonBatchGenerateResponse(
            results=results, total_processing_time=total_elapsed
        )

    except Exception as e:
        logger.error(f"[generate-batch] Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# GPU ìµœì í™” ì ìš©
# ============================================================================


def apply_gpu_optimizations():
    """GPU ìµœì í™” ì ìš©"""
    global GPU_OPTIMIZATIONS_ENABLED, device

    # Device ëª…ì‹œì  ì„¤ì •
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ“ Using device: {device_str}")

    logger.info("=" * 80)
    logger.info("ğŸš€ Applying GPU Optimizations...")
    logger.info("=" * 80)

    try:
        # Gradio ìŠ¤íƒ€ì¼: ìµœì†Œí•œì˜ ìµœì í™”ë§Œ ì‚¬ìš©
        # 0. íŒŒì´í”„ë¼ì¸ì„ GPUë¡œ ì´ë™ (float16 ì‚¬ìš©)
        logger.info("0ï¸âƒ£ Moving pipeline to GPU (float16)...")
        pipe.to(device)
        logger.info(f"âœ… Pipeline moved to {device}")

        # ëª¨ë“  ìµœì í™” ë¹„í™œì„±í™” - ìˆœìˆ˜ Gradio ìŠ¤íƒ€ì¼
        logger.info("ğŸ“ Pure Gradio style: NO optimizations")
        logger.info("   Skipping: xFormers, torch.compile, cuDNN benchmark, TF32")
        logger.info("   Using only: float16 + autocast")

        GPU_OPTIMIZATIONS_ENABLED = True
        logger.info("=" * 80)
        logger.info("ğŸ‰ GPU Optimizations Applied Successfully!")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"âŒ GPU optimization failed: {e}", exc_info=True)
        logger.warning("âš ï¸  Continuing without optimizations...")


# ============================================================================
# ì„œë²„ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("VTON_PORT", "8001"))

    logger.info("=" * 80)
    logger.info("âœ… IDM-VTON Models Loaded Successfully!")
    logger.info(f"ğŸš€ Starting FastAPI server on port {port}...")
    logger.info("Production mode: S3-based caching (no memory cache)")
    logger.info("=" * 80)

    # GPU ìµœì í™” ì ìš©
    apply_gpu_optimizations()

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        timeout_keep_alive=75,  # Keep-alive íƒ€ì„ì•„ì›ƒ ì¦ê°€
        backlog=100,  # ëŒ€ê¸°ì—´ í¬ê¸° ì¦ê°€ (íì‰ ì§€ì›)
    )
