"""
IDM-VTON FastAPI Server
ê¸°ì¡´ IDM-VTON ëª¨ë¸ì„ FastAPIë¡œ ê°ì‹¸ì„œ ì œê³µ
ì„¤ì¹˜ ìœ„ì¹˜: ~/app/virtual-try/IDM-VTON/api_server.py
"""

import sys
import os

# IDM-VTON gradio_demo ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
IDMVTON_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(IDMVTON_ROOT, 'gradio_demo'))

# ============================================================================
# IDM-VTON ëª¨ë¸ ì´ˆê¸°í™” (app_optimized.pyì˜ init_code ë¶€ë¶„)
# ============================================================================
print("=" * 80)
print("ğŸš€ Initializing IDM-VTON models...")
print("=" * 80)

with open('gradio_demo/app.py', 'r') as f:
    app_code = f.read()

# ëª¨ë¸ ë¡œë”© ì½”ë“œ ì¶”ì¶œ ë° ì‹¤í–‰
init_code = app_code.split('def start_tryon')[0].split('garm_list = os.listdir')[0]
exec(init_code)

import numpy as np
import time
import io
import base64
from PIL import Image
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="IDM-VTON API Server", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# ìºì‹œ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬)
# ============================================================================
human_cache = {}
garment_cache = {}
text_cache = {}

# ============================================================================
# Request/Response Models
# ============================================================================

class HumanPreprocessRequest(BaseModel):
    image_base64: str

class HumanPreprocessResponse(BaseModel):
    human_img: str
    mask: str
    mask_gray: str
    pose_img_tensor: str  # "cached" ë¬¸ìì—´ë¡œ ë°˜í™˜ (ì‹¤ì œ í…ì„œëŠ” ë©”ëª¨ë¦¬ì—)

class GarmentPreprocessRequest(BaseModel):
    image_base64: str

class GarmentPreprocessResponse(BaseModel):
    garm_img: str
    garm_tensor: str  # "cached" ë¬¸ìì—´ë¡œ ë°˜í™˜

class TextPreprocessRequest(BaseModel):
    garment_description: str

class TextPreprocessResponse(BaseModel):
    prompt_embeds: str
    negative_prompt_embeds: str
    pooled_prompt_embeds: str
    negative_pooled_prompt_embeds: str
    prompt_embeds_c: str

class VtonGenerateRequest(BaseModel):
    user_id: str
    clothing_id: str
    denoise_steps: int = 20
    seed: int = 42

class VtonGenerateResponse(BaseModel):
    result_image_base64: str
    processing_time: float

# ============================================================================
# Helper Functions
# ============================================================================

def base64_to_pil(base64_str: str) -> Image.Image:
    """Base64 â†’ PIL Image"""
    image_data = base64.b64decode(base64_str)
    image = Image.open(io.BytesIO(image_data))
    return image

def pil_to_base64(pil_img: Image.Image) -> str:
    """PIL Image â†’ Base64"""
    buffered = io.BytesIO()
    pil_img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

# ============================================================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜ (app_optimized.py ê¸°ë°˜)
# ============================================================================

def preprocess_human_internal(human_img: Image.Image, human_id: str):
    """ì‚¬ëŒ ì „ì²˜ë¦¬ - OpenPose + Parsing + DensePose"""
    logger.info(f"â³ Preprocessing human: {human_id}")
    start = time.time()

    if isinstance(human_img, np.ndarray):
        human_img = Image.fromarray(human_img)

    human_img = human_img.convert("RGB").resize((768, 1024))
    human_img_arg = _apply_exif_orientation(human_img.resize((384, 512)))
    human_img_arg = convert_PIL_to_numpy(human_img_arg, format="BGR")

    args = apply_net.create_argument_parser().parse_args(
        ['show', './configs/densepose_rcnn_R_50_FPN_s1x.yaml',
         './ckpt/densepose/model_final_162be9.pkl', 'dp_segm', '-v',
         '--opts', 'MODEL.DEVICE', 'cuda']
    )

    # OpenPose
    keypoints = openpose_model(human_img.resize((384, 512)))

    # Parsing
    model_parse, _ = parsing_model(human_img.resize((384, 512)))
    mask, mask_gray = get_mask_location('hd', "upper_body", model_parse, keypoints)
    mask = mask.resize((768, 1024))

    # DensePose
    pose_img = args.func(args, human_img_arg)
    pose_img = pose_img[:,:,::-1]
    pose_img = Image.fromarray(pose_img).resize((768, 1024))
    pose_img_tensor = tensor_transfrom(pose_img).unsqueeze(0).to(device, torch.float16)

    # ìºì‹œ ì €ì¥
    human_cache[human_id] = {
        'human_img': human_img,
        'mask': mask,
        'mask_gray': mask_gray,
        'pose_img_tensor': pose_img_tensor,
    }

    elapsed = time.time() - start
    logger.info(f"âœ… Human {human_id} cached in {elapsed:.2f}s")

    return {
        'human_img': pil_to_base64(human_img),
        'mask': pil_to_base64(mask),
        'mask_gray': pil_to_base64(mask_gray),
        'pose_img_tensor': "cached",
        'elapsed': elapsed,
    }

def preprocess_garment_internal(garm_img: Image.Image, garment_id: str):
    """ì˜· ì „ì²˜ë¦¬"""
    logger.info(f"â³ Preprocessing garment: {garment_id}")
    start = time.time()

    if isinstance(garm_img, np.ndarray):
        garm_img = Image.fromarray(garm_img)

    garm_img = garm_img.convert("RGB").resize((768, 1024))
    garm_img_resized = garm_img.resize((384, 512))
    garm_tensor = tensor_transfrom(garm_img_resized).unsqueeze(0).to(device, torch.float16)

    # ìºì‹œ ì €ì¥
    garment_cache[garment_id] = {
        'garm_img': garm_img,
        'garm_tensor': garm_tensor,
    }

    elapsed = time.time() - start
    logger.info(f"âœ… Garment {garment_id} cached in {elapsed:.2f}s")

    return {
        'garm_img': pil_to_base64(garm_img),
        'garm_tensor': "cached",
        'elapsed': elapsed,
    }

def preprocess_text_internal(garment_des: str):
    """í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
    if garment_des in text_cache:
        logger.info(f"âœ… Text already cached: '{garment_des}'")
        return {'status': 'already_cached', 'elapsed': 0}

    logger.info(f"â³ Encoding text: '{garment_des}'")
    start = time.time()

    prompt = "model wearing " + garment_des
    prompt_c = "a photo of " + garment_des
    negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

    with torch.no_grad():
        pipe.to(device)
        original_dtype = pipe.text_encoder.dtype
        pipe.text_encoder.to(torch.float32)
        pipe.text_encoder_2.to(torch.float32)

        prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds = pipe.encode_prompt(
            prompt, num_images_per_prompt=1, do_classifier_free_guidance=True, negative_prompt=negative_prompt
        )

        prompt_embeds_c, _, _, _ = pipe.encode_prompt(
            prompt_c, num_images_per_prompt=1, do_classifier_free_guidance=False, negative_prompt=negative_prompt
        )

        pipe.text_encoder.to(original_dtype)
        pipe.text_encoder_2.to(original_dtype)
        pipe.to(device)

    # ìºì‹œ ì €ì¥
    text_cache[garment_des] = {
        'prompt_embeds': prompt_embeds.to(device, torch.float16),
        'negative_prompt_embeds': negative_prompt_embeds.to(device, torch.float16),
        'pooled_prompt_embeds': pooled_prompt_embeds.to(device, torch.float16),
        'negative_pooled_prompt_embeds': negative_pooled_prompt_embeds.to(device, torch.float16),
        'prompt_embeds_c': prompt_embeds_c.to(device, torch.float16),
    }

    elapsed = time.time() - start
    logger.info(f"âœ… Text cached in {elapsed:.2f}s")

    return {'status': 'cached', 'elapsed': elapsed}

def generate_tryon_internal(human_id: str, garment_id: str, garment_des: str, denoise_steps: int, seed: int):
    """ì‹¤ì‹œê°„ ìƒì„± - Diffusionë§Œ ì‹¤í–‰"""
    if human_id not in human_cache:
        raise ValueError(f"Human '{human_id}' not found in cache")
    if garment_id not in garment_cache:
        raise ValueError(f"Garment '{garment_id}' not found in cache")
    if garment_des not in text_cache:
        raise ValueError(f"Text '{garment_des}' not cached")

    logger.info(f"âš¡ Generating: {human_id} + {garment_id}")
    start = time.time()

    human_data = human_cache[human_id]
    garment_data = garment_cache[garment_id]
    text_data = text_cache[garment_des]

    with torch.no_grad():
        generator = torch.Generator(device).manual_seed(int(seed))

        images = pipe(
            prompt_embeds=text_data['prompt_embeds'],
            negative_prompt_embeds=text_data['negative_prompt_embeds'],
            pooled_prompt_embeds=text_data['pooled_prompt_embeds'],
            negative_pooled_prompt_embeds=text_data['negative_pooled_prompt_embeds'],
            num_inference_steps=int(denoise_steps),
            generator=generator,
            strength=1.0,
            pose_img=human_data['pose_img_tensor'],
            text_embeds_cloth=text_data['prompt_embeds_c'],
            cloth=garment_data['garm_tensor'],
            mask_image=human_data['mask'],
            image=human_data['human_img'],
            height=1024,
            width=768,
            ip_adapter_image=garment_data['garm_img'],
            guidance_scale=2.0,
        )[0]

    elapsed = time.time() - start
    logger.info(f"âš¡ Generated in {elapsed:.2f}s")

    return images[0], elapsed

# ============================================================================
# API Endpoints
# ============================================================================

@app.get("/")
def root():
    return {
        "service": "IDM-VTON API Server",
        "status": "running",
        "port": 8001,
        "environment": "conda",
        "models_loaded": True,
        "cache_stats": {
            "humans": len(human_cache),
            "garments": len(garment_cache),
            "texts": len(text_cache),
        }
    }

@app.get("/health")
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "models_loaded": True,
        "cache_stats": {
            "humans": len(human_cache),
            "garments": len(garment_cache),
            "texts": len(text_cache),
        }
    }

@app.post("/vton/preprocess-human", response_model=HumanPreprocessResponse)
async def preprocess_human(request: HumanPreprocessRequest):
    """
    ì‚¬ëŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: OpenPose + Parsing + DensePose
    Cache Key: user_id
    """
    try:
        # Base64 â†’ PIL
        human_img = base64_to_pil(request.image_base64)

        # ì„ì‹œ ID ìƒì„± (ì‹¤ì œë¡œëŠ” NestJSì—ì„œ user_idë¥¼ URL íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•¨)
        # í˜„ì¬ëŠ” ì´ë¯¸ì§€ í•´ì‹œë¥¼ IDë¡œ ì‚¬ìš©
        human_id = f"human_{hash(request.image_base64) % 1000000}"

        # ì „ì²˜ë¦¬
        result = preprocess_human_internal(human_img, human_id)

        return HumanPreprocessResponse(
            human_img=result['human_img'],
            mask=result['mask'],
            mask_gray=result['mask_gray'],
            pose_img_tensor=result['pose_img_tensor'],
        )
    except Exception as e:
        logger.error(f"Error in preprocess_human: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/vton/preprocess-garment", response_model=GarmentPreprocessResponse)
async def preprocess_garment(request: GarmentPreprocessRequest):
    """
    ì˜· ì´ë¯¸ì§€ ì „ì²˜ë¦¬: ë¦¬ì‚¬ì´ì¦ˆ + í…ì„œ ë³€í™˜
    Cache Key: clothing_id
    """
    try:
        # Base64 â†’ PIL
        garm_img = base64_to_pil(request.image_base64)

        # ì„ì‹œ ID ìƒì„±
        garment_id = f"garment_{hash(request.image_base64) % 1000000}"

        # ì „ì²˜ë¦¬
        result = preprocess_garment_internal(garm_img, garment_id)

        return GarmentPreprocessResponse(
            garm_img=result['garm_img'],
            garm_tensor=result['garm_tensor'],
        )
    except Exception as e:
        logger.error(f"Error in preprocess_garment: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/vton/preprocess-text", response_model=TextPreprocessResponse)
async def preprocess_text(request: TextPreprocessRequest):
    """
    í…ìŠ¤íŠ¸ ì¸ì½”ë”©: CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©
    Cache Key: garment_description
    """
    try:
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        result = preprocess_text_internal(request.garment_description)

        return TextPreprocessResponse(
            prompt_embeds="cached",
            negative_prompt_embeds="cached",
            pooled_prompt_embeds="cached",
            negative_pooled_prompt_embeds="cached",
            prompt_embeds_c="cached",
        )
    except Exception as e:
        logger.error(f"Error in preprocess_text: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/vton/generate-tryon", response_model=VtonGenerateResponse)
async def generate_tryon(request: VtonGenerateRequest):
    """
    ìºì‹œëœ ë°ì´í„°ë¡œ Diffusion ì‹¤í–‰

    Note: í˜„ì¬ êµ¬í˜„ì€ ì„ì‹œ í•´ì‹œ IDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì‹¤ì œë¡œëŠ” S3ì—ì„œ ìºì‹œë¥¼ ë¡œë“œí•˜ê±°ë‚˜, user_id/clothing_idë¥¼ ì§ì ‘ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        # ì„ì‹œ: user_idì™€ clothing_idë¥¼ ê·¸ëŒ€ë¡œ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
        human_id = request.user_id
        garment_id = request.clothing_id

        # ê¸°ë³¸ description (ì‹¤ì œë¡œëŠ” S3ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•¨)
        garment_des = "a shirt"  # TODO: ì‹¤ì œ description ê°€ì ¸ì˜¤ê¸°

        # ìƒì„±
        result_img, elapsed = generate_tryon_internal(
            human_id, garment_id, garment_des, request.denoise_steps, request.seed
        )

        # PIL â†’ Base64
        result_base64 = pil_to_base64(result_img)

        return VtonGenerateResponse(
            result_image_base64=result_base64,
            processing_time=elapsed,
        )
    except ValueError as e:
        logger.error(f"Cache miss: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error in generate_tryon: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# S3 ìºì‹œ ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ (í–¥í›„ êµ¬í˜„)
# ============================================================================

@app.post("/vton/load-cache-from-s3")
async def load_cache_from_s3(user_id: str, clothing_id: str):
    """
    S3ì—ì„œ ìºì‹œ ë°ì´í„° ë¡œë“œ

    TODO:
    1. S3ì—ì„œ users/{user_id}/vton-cache/*.pkl ë‹¤ìš´ë¡œë“œ
    2. PyTorch í…ì„œ ì—­ì§ë ¬í™”
    3. ë©”ëª¨ë¦¬ ìºì‹œì— ë¡œë“œ
    """
    raise HTTPException(status_code=501, detail="Not implemented yet")

# ============================================================================
# ì„œë²„ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("VTON_PORT", "8001"))

    logger.info("=" * 80)
    logger.info("âœ… IDM-VTON Models Loaded Successfully!")
    logger.info(f"ğŸš€ Starting FastAPI server on port {port}...")
    logger.info("=" * 80)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
